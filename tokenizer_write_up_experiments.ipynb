{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e20a39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('牛')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8532a4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'牛'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(29275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997e0a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! this is is a test'\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! this is is a test\"\n",
    "utf8_encoded = test_string.encode('utf-8')\n",
    "print(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549736a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "[104, 101, 108, 108, 111, 33, 32, 116, 104, 105, 115, 32, 105, 115, 32, 105, 115, 32, 97, 32, 116, 101, 115, 116]\n"
     ]
    }
   ],
   "source": [
    "print(type(utf8_encoded))\n",
    "print(list(map(int,utf8_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181cb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import regex as re\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, corpus: str, vocab_size: int, special_tokens: list[str]) -> None:\n",
    "        self.corpus: str = corpus\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.special_tokens: list[str] = special_tokens\n",
    "\n",
    "        self.PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "        self.vocab: dict[int, bytes] = {i: bytes([i]) for i in range(256)}\n",
    "        self.merge_sets: dict[int, tuple[int, int]] = {}\n",
    "\n",
    "        self.special_token_to_id: dict[str, int] = {}\n",
    "\n",
    "        # Add special tokens to vocab\n",
    "        for i, token in enumerate(special_tokens):\n",
    "            token_id = 256 + i  # reserve IDs above byte values\n",
    "            self.vocab[token_id] = token.encode(\"utf-8\")\n",
    "            self.special_token_to_id[token] = token_id\n",
    "\n",
    "        self.next_token_id = 256 + len(special_tokens)\n",
    "\n",
    "        self.pretoken_table_count: dict[tuple[int, ...], int] = self.get_pre_token_freq_table()\n",
    "\n",
    "\n",
    "    def get_pretoken(self) -> list[str]:\n",
    "        return re.findall(self.PAT, self.corpus)\n",
    "\n",
    "    def get_pre_token_freq_table(self) -> dict[tuple[int, ...], int]:\n",
    "        freq_pre_tokens: dict[tuple[int, ...], int] = defaultdict(int)\n",
    "        for pre_token in self.get_pretoken():\n",
    "            pretoken_stream = tuple(pre_token.encode(\"utf-8\"))\n",
    "            freq_pre_tokens[pretoken_stream] += 1\n",
    "        return freq_pre_tokens\n",
    "\n",
    "    def get_token_freq_pairs(self) -> tuple[dict[tuple[int, int], int], dict[tuple[int, int], list[tuple[int, ...]]]]:\n",
    "        token_pairs: dict[tuple[int, int], int] = defaultdict(int)\n",
    "        index_table: dict[tuple[int, int], list[tuple[int, ...]]] = defaultdict(list)\n",
    "\n",
    "        for pretoken, freq in self.pretoken_table_count.items():\n",
    "            for token1, token2 in zip(pretoken, pretoken[1:]):\n",
    "                token_pairs[(token1, token2)] += freq\n",
    "                index_table[(token1, token2)].append(pretoken)\n",
    "\n",
    "        return token_pairs, index_table\n",
    "\n",
    "    def update_pre_token_table(self, most_frequent: tuple[int, int], pretokens_appeared: list[tuple[int, ...]]) -> None:\n",
    "        token1, token2 = most_frequent\n",
    "        new_token_id = max(self.vocab.keys()) + 1\n",
    "        new_token_bytes = self.vocab[token1] + self.vocab[token2]\n",
    "\n",
    "        self.vocab[new_token_id] = new_token_bytes\n",
    "        self.merge_sets[new_token_id] = (token1, token2)\n",
    "\n",
    "        for pretoken in pretokens_appeared:\n",
    "            freq = self.pretoken_table_count[pretoken]\n",
    "            new_pre_token = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(pretoken):\n",
    "                if i < len(pretoken) - 1 and pretoken[i] == token1 and pretoken[i + 1] == token2:\n",
    "                    new_pre_token.append(new_token_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_pre_token.append(pretoken[i])\n",
    "                    i += 1\n",
    "\n",
    "            del self.pretoken_table_count[pretoken]\n",
    "            self.pretoken_table_count[tuple(new_pre_token)] += freq\n",
    "\n",
    "    def merge(self) -> None:\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freq, pair_index = self.get_token_freq_pairs()\n",
    "\n",
    "            if not pair_freq:\n",
    "                break\n",
    "\n",
    "            most_frequent = max(pair_freq.items(), key=lambda x: x[1])[0]\n",
    "            pretokens_appeared = pair_index[most_frequent]\n",
    "\n",
    "            self.update_pre_token_table(most_frequent, pretokens_appeared)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Tokenizes new input text using the trained merge rules.\n",
    "        \"\"\"\n",
    "        pre_tokens = re.findall(self.PAT, text)\n",
    "        encoded_tokens = []\n",
    "\n",
    "        for token in pre_tokens:\n",
    "            byte_sequence = list(token.encode(\"utf-8\"))\n",
    "\n",
    "            # Apply merges to byte sequence\n",
    "            byte_sequence = self.apply_merges(byte_sequence)\n",
    "\n",
    "            encoded_tokens.extend(byte_sequence)\n",
    "\n",
    "        return encoded_tokens\n",
    "\n",
    "    def apply_merges(self, byte_sequence: list[int]) -> list[int]:\n",
    "        \"\"\"\n",
    "        Applies learned BPE merges to a sequence of byte-level token IDs.\n",
    "        \"\"\"\n",
    "        merges = list(self.merge_sets.items())\n",
    "        merges.sort()  # Ensure order of application (token ID ascending)\n",
    "\n",
    "        for token_id, (a, b) in merges:\n",
    "            i = 0\n",
    "            new_sequence = []\n",
    "            while i < len(byte_sequence):\n",
    "                if i < len(byte_sequence) - 1 and byte_sequence[i] == a and byte_sequence[i + 1] == b:\n",
    "                    new_sequence.append(token_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_sequence.append(byte_sequence[i])\n",
    "                    i += 1\n",
    "            byte_sequence = new_sequence\n",
    "\n",
    "        return byte_sequence\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Converts token IDs back into a string using the vocab mapping.\n",
    "        \"\"\"\n",
    "        byte_stream = b''.join(self.vocab[token] for token in tokens)\n",
    "        return byte_stream.decode(\"utf-8\", errors=\"replace\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62982dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the new text the same thing as the decoded? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = f\"LOUISVILLE, Ky. — A few unflattering reviews are to be expected with any hotel. The lobby of Hotel Louisville Pat McDonogh for Al Jazeera America Every homeless shelter has a NIMBY problem. Try building a new facility or renovating an old one and the neighbors come out of the woodwork to protest each additional bed. But the battle waged against Hotel Louisville was unusual even in the long history of Wayside Christian Mission, founded in 1957. The saga began six years ago, after the group finally raised enough money to replace its worn-out transitional-housing facility for women and kids. Initially, the married couple at Wayside’s helm — Tim Moseley, a bearded, heavyset minister, and his wife, Nina, an attorney with waist-length platinum blonde hair — intended to build on property it already owned along gentrifying Market Street. Real-estate developers with city-hall ties killed the plan, claiming the need for\"\n",
    "bpe = BPE(corpus, 300, [])\n",
    "bpe.merge()\n",
    "new_text = f\"Then, in early 2009, the Moseleys heard that the downtown Holiday Inn, nicknamed “Hotel Louisville,” would be sold at a foreclosure auction. The final price tag of $10 million depleted all the funds Wayside had raised through its years-long capital campaign and proceeds from the Market Street sale, but at 187 rooms and 169,400 square feet, the building could house hundreds. Eighty-three homeless women moved into the hotel in November. Shortly thereafter, with utility costs mounting and many floors vacant, the Moseleys saw an opportunity. “People kept coming through and asking for a room,” Nina Moseley recalled. So Wayside opened Hotel Louisville to the public while continuing to provide shelter and substance-abuse recovery services to women in need, free of charge.\"\n",
    "encoded = bpe.encode(new_text)\n",
    "decoded = bpe.decode(encoded)\n",
    "print(f\"is the new text the same thing as the decoded? {new_text == decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca463384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    result = pool.map(square, [1, 2, 3, 4])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8a335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 running\n",
      "Task 1 running\n",
      "Task 2 running\n",
      "Task 3 running\n",
      "Task 4 running\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "def task(n):\n",
    "    print(f\"Task {n} running\")\n",
    "\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    t = threading.Thread(target=task, args=(i,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f590a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 running\n",
      "Process 1 running\n",
      "Process 2 running\n",
      "Process 3 running\n",
      "Process 4 running\n",
      "the number of CPU in your computer is: 8\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, cpu_count\n",
    "\n",
    "def task(n):\n",
    "    print(f\"Process {n} running\")\n",
    "\n",
    "processes = []\n",
    "for i in range(5):\n",
    "    p = Process(target=task, args=(i,))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "print(f\"the number of CPU in your computer is: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92d202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Task 0 done', 'Task 1 done', 'Task 2 done', 'Task 3 done', 'Task 4 done']\n",
      "['Task 0 done', 'Task 1 done', 'Task 2 done', 'Task 3 done', 'Task 4 done']\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "def task(n):\n",
    "    return f\"Task {n} done\"\n",
    "\n",
    "# For concurrency\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(task, range(5))\n",
    "    print(list(results))\n",
    "\n",
    "# For parallelism\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(task, range(5))\n",
    "    print(list(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258d8be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
